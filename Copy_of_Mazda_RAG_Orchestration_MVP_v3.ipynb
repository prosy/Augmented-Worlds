{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prosy/Augmented-Worlds/blob/main/Copy_of_Mazda_RAG_Orchestration_MVP_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609cf30a",
      "metadata": {
        "id": "609cf30a"
      },
      "source": [
        "# Mazda Owner's Manual RAG — **MVP Orchestration Notebook (v3)**\n",
        "\n",
        "**Goal:** Keep Mazda-specific structure, but include a tiny in-notebook BM25 test and clear comments for swapping to FAISS/ColBERT.\n",
        "Outputs export to `/app/` so your live app can use them immediately."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe1f8cc",
      "metadata": {
        "id": "3fe1f8cc"
      },
      "source": [
        "## 0) Setup (install if needed)\n",
        "\n",
        "Notebook is dependency-light. You can run as-is for the demo (BM25-ish). If you want FAISS/ColBERT, install and wire in the optional cells later."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976b6eae",
      "metadata": {
        "id": "976b6eae"
      },
      "source": [
        "## 1) Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, csv, re, time, shutil, math\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "r6k5UnuTsqY0"
      },
      "id": "r6k5UnuTsqY0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Config Paths"
      ],
      "metadata": {
        "id": "vo1ouTEcMl8p"
      },
      "id": "vo1ouTEcMl8p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db6122c",
      "metadata": {
        "id": "9db6122c"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Set project output root explicitly\n",
        "WORK_ROOT = Path(\"/content/drive/MyDrive/AugWorlds/Mazda_PDFs/\")\n",
        "\n",
        "# Define subdirectories inside Mazda_PDFs for pipeline outputs\n",
        "PARSED_DIR = WORK_ROOT / \"parsed\"\n",
        "ENRICHED_DIR = WORK_ROOT / \"enriched\"\n",
        "INDEX_DIR = WORK_ROOT / \"index\"\n",
        "APP_DIR = WORK_ROOT / \"app\"  # app configs also stored here\n",
        "\n",
        "# Mazda-specific input PDF\n",
        "INPUT_PDF = WORK_ROOT / \"2024-cx-50-owners-manual.pdf\"  # Mazda PDF lives in same folder\n",
        "MODEL_YEAR = \"2024\"  # for configs if needed\n",
        "\n",
        "# Config files used by the live app\n",
        "SYNONYMS_CSV = APP_DIR / \"config\" / \"synonyms.csv\"\n",
        "ANS_PLANS_JSON = APP_DIR / \"config\" / \"answer_plans.json\"\n",
        "RETRIEVAL_CFG = APP_DIR / \"config\" / \"retrieval_config.yaml\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "for d in [PARSED_DIR, ENRICHED_DIR, INDEX_DIR, APP_DIR, APP_DIR / \"config\"]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"WORK_ROOT:\", WORK_ROOT)\n",
        "print(\"INPUT_PDF:\", INPUT_PDF)\n",
        "\n",
        "if INPUT_PDF.is_file():\n",
        "    print(f\"✅ Found Mazda PDF: {INPUT_PDF}\")\n",
        "else:\n",
        "    print(f\"❌ PDF not found or path incorrect: {INPUT_PDF}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a245a2",
      "metadata": {
        "id": "e0a245a2"
      },
      "source": [
        "## 3) Parsing: chunk pages + harvest TOC lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112e5084",
      "metadata": {
        "id": "112e5084"
      },
      "outputs": [],
      "source": [
        "# Replace with your real parser (PyMuPDF/Tika). Demo JSONL for structure.\n",
        "\n",
        "# Helper function to extract metadata from filename\n",
        "def extract_metadata_from_filename(pdf_path):\n",
        "    import re\n",
        "    import os\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    # Updated pattern to handle more filename variations\n",
        "    patterns = [\n",
        "        r'(\\d{4})-([a-z0-9]+)-owners-manual',\n",
        "        r'(\\d{4})-mazda([a-z0-9]+)-',\n",
        "        r'(\\d{4})-([a-z0-9-]+)-owners'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, filename.lower())\n",
        "        if match:\n",
        "            year, model_match = match.groups()\n",
        "            # Clean up model string\n",
        "            model = re.sub(r'[^a-z0-9]', '', model_match)\n",
        "            return {\"filename\": filename, \"year\": int(year), \"model\": model, \"page_count\": None}\n",
        "\n",
        "    # Default return if no pattern matches\n",
        "    return {\"filename\": filename, \"year\": None, \"model\": None, \"page_count\": None}\n",
        "\n",
        "\n",
        "# Find all PDF files in the WORK_ROOT directory\n",
        "pdf_files = list(WORK_ROOT.glob(\"*.pdf\"))\n",
        "\n",
        "processed_parsed_files = []\n",
        "\n",
        "if not pdf_files:\n",
        "    print(f\"No PDF files found in {WORK_ROOT}. Please ensure your PDF(s) are in this directory.\")\n",
        "else:\n",
        "    print(f\"Found {len(pdf_files)} PDF file(s) to process.\")\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Processing file: {pdf_file.name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Extract metadata from the filename\n",
        "        metadata = extract_metadata_from_filename(pdf_file)\n",
        "        print(f\"Extracted metadata: {metadata}\")\n",
        "\n",
        "        # Define the output path for the parsed data for THIS file\n",
        "        # Use extracted metadata to name the output file\n",
        "        year_str = metadata.get('year', 'unknown')\n",
        "        model_str = metadata.get('model', 'unknown')\n",
        "        PARSED_JSONL_FOR_FILE = PARSED_DIR / f\"sections_{year_str}_{model_str}.jsonl\"\n",
        "        processed_parsed_files.append(PARSED_JSONL_FOR_FILE)\n",
        "\n",
        "\n",
        "        # Replace this with your actual PDF parsing logic that would return sections for this file\n",
        "        # For the demo, we'll create sample data based on the metadata\n",
        "        file_demo_sections = [\n",
        "            {\"manual_ref\": \"1-1\", \"title\": f\"Introduction ({metadata.get('year')} {metadata.get('model')})\", \"text\": f\"Welcome to your {metadata.get('year')} {metadata.get('model')} Mazda owner's manual.\", \"metadata\": metadata},\n",
        "            {\"manual_ref\": \"2-14\", \"title\": \"TPMS\", \"text\": \"TPMS monitors tire pressure and warns if it is low.\", \"metadata\": metadata},\n",
        "            {\"manual_ref\": \"3-2\", \"title\": \"Engine Oil\", \"text\": \"Use 0W-20. Check level regularly.\", \"metadata\": metadata},\n",
        "        ]\n",
        "\n",
        "        # Write demo sections for THIS file to its specific JSONL file\n",
        "        with open(PARSED_JSONL_FOR_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            for i, row in enumerate(file_demo_sections):\n",
        "                # Assign a unique _id for sections within this file (or a global one if preferred later)\n",
        "                row[\"_id\"] = i\n",
        "                f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "        print(f\"Parsed (using demo data and filename metadata) → {PARSED_JSONL_FOR_FILE}\")\n",
        "\n",
        "    # Store the list of processed parsed files for the next step\n",
        "    # This could be saved to a file or passed as a variable\n",
        "    # For now, we'll rely on the next cell finding files in PARSED_DIR\n",
        "    print(\"\\nFinished processing all PDF files with demo data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cc90bbc",
      "metadata": {
        "id": "1cc90bbc"
      },
      "source": [
        "## 4) Enrichment: fill `manual_ref` for Top-20 plans (per year/manual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9269cc29",
      "metadata": {
        "id": "9269cc29"
      },
      "outputs": [],
      "source": [
        "def load_synonyms(path=SYNONYMS_CSV):\n",
        "    norm = {}\n",
        "    if path.exists():\n",
        "        import csv\n",
        "        with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "            for row in csv.DictReader(f):\n",
        "                canonical = row[\"canonical\"].strip()\n",
        "                for alias in row[\"alias_list\"].split(\"|\"):\n",
        "                    norm[alias.strip().lower()] = canonical.lower()\n",
        "    return norm\n",
        "\n",
        "def normalize_text(text: str, norm_map: dict):\n",
        "    out = text\n",
        "    for alias, canon in norm_map.items():\n",
        "        out = re.sub(rf\"\\b{re.escape(alias)}\\b\", canon, out, flags=re.IGNORECASE)\n",
        "    return out\n",
        "\n",
        "norm_map = load_synonyms()\n",
        "\n",
        "# Find all parsed JSONL files created in Section 2\n",
        "parsed_files = list(PARSED_DIR.glob(\"sections_*.jsonl\"))\n",
        "\n",
        "if not parsed_files:\n",
        "    print(f\"No parsed files found in {PARSED_DIR}. Please run Section 2 first.\")\n",
        "else:\n",
        "    print(f\"Found {len(parsed_files)} parsed file(s) to enrich.\")\n",
        "\n",
        "    for parsed_file in parsed_files:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Enriching file: {parsed_file.name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Define the output path for the enriched data for THIS file\n",
        "        enriched_file_name = parsed_file.name.replace(\".jsonl\", \".enriched.jsonl\")\n",
        "        ENRICHED_JSONL_FOR_FILE = ENRICHED_DIR / enriched_file_name\n",
        "\n",
        "        with open(parsed_file, \"r\", encoding=\"utf-8\") as fin, open(ENRICHED_JSONL_FOR_FILE, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for line in fin:\n",
        "                rec = json.loads(line)\n",
        "                rec[\"text_norm\"] = normalize_text(rec[\"text\"], norm_map)\n",
        "                # Metadata is already included from Section 2\n",
        "                fout.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "        print(f\"Enriched → {ENRICHED_JSONL_FOR_FILE}\")\n",
        "\n",
        "    print(\"\\nFinished enriching all parsed files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6159f338",
      "metadata": {
        "id": "6159f338"
      },
      "source": [
        "## 5) Indexing: build lightweight retrieval artifacts (BM25-ish demo)\n",
        "\n",
        "*This cell provides an in-notebook BM25-like index so the notebook works out-of-the-box.*\n",
        "\n",
        "**Swap to FAISS/ColBERT**:\n",
        "- FAISS: create vectors for `text_norm`, save `faiss.index` + `docs.jsonl` under `app/index/`.\n",
        "- ColBERT: run your existing CLI to build an index and then point to it via `app/config/retrieval_config.yaml`.\n",
        "Keep the *calling interface* below so your app code remains unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "63d00ee5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63d00ee5",
        "outputId": "df11b994-7c0b-4cbc-f73d-6c0bcf1123cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13 enriched file(s) to index.\n",
            "Loading data from sections_2024.enriched.jsonl...\n",
            "Loading data from sections_combined_2024.enriched.jsonl...\n",
            "Loading data from sections_2023_cx9.enriched.jsonl...\n",
            "Loading data from sections_2025_3.enriched.jsonl...\n",
            "Loading data from sections_2025_cx70.enriched.jsonl...\n",
            "Loading data from sections_2025_cx70phev.enriched.jsonl...\n",
            "Loading data from sections_2025_cx90.enriched.jsonl...\n",
            "Loading data from sections_2025_cx90phev.enriched.jsonl...\n",
            "Loading data from sections_2025_cx30vehicle.enriched.jsonl...\n",
            "Loading data from sections_2023_cx30.enriched.jsonl...\n",
            "Loading data from sections_2024_cx50.enriched.jsonl...\n",
            "Loading data from sections_2023_3.enriched.jsonl...\n",
            "Loading data from sections_2022_cx9.enriched.jsonl...\n",
            "Index written → /content/drive/MyDrive/AugWorlds/Mazda_PDFs/index/bm25_index_combined.json\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "import json\n",
        "import math\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# Define output paths for the index and docs files (using a generic name for combined data)\n",
        "INDEX_JSON = INDEX_DIR / \"bm25_index_combined.json\"\n",
        "DOCS_JSON = INDEX_DIR / \"docs_combined.jsonl\"\n",
        "\n",
        "# Load docs from all enriched files\n",
        "docs = []\n",
        "enriched_files = list(ENRICHED_DIR.glob(\"*.enriched.jsonl\"))\n",
        "\n",
        "if not enriched_files:\n",
        "    print(f\"No enriched files found in {ENRICHED_DIR}. Please run Section 3 first.\")\n",
        "else:\n",
        "    print(f\"Found {len(enriched_files)} enriched file(s) to index.\")\n",
        "    for enriched_file in enriched_files:\n",
        "        print(f\"Loading data from {enriched_file.name}...\")\n",
        "        with open(enriched_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                rec = json.loads(line)\n",
        "                # Ensure a unique _id across all documents if needed,\n",
        "                # or rely on the original _id if it's unique per manual and you handle it later.\n",
        "                # For simplicity here, we'll re-assign a global _id.\n",
        "                docs.append(rec)\n",
        "\n",
        "# Re-assign global _ids after loading all documents\n",
        "for i, doc in enumerate(docs):\n",
        "    doc[\"_id\"] = i\n",
        "\n",
        "N = len(docs)\n",
        "df = defaultdict(int)\n",
        "postings = defaultdict(list)   # term -> list of (doc_id, tf)\n",
        "doc_len = {}\n",
        "\n",
        "# Re-implement tokenize function here to ensure it's available\n",
        "def tokenize(s):\n",
        "    return re.findall(r\"[a-z0-9_]+\", s.lower())\n",
        "\n",
        "for d in docs:\n",
        "    tokens = tokenize(d[\"text_norm\"])\n",
        "    doc_len[d[\"_id\"]] = len(tokens)\n",
        "    tf = defaultdict(int)\n",
        "    for t in tokens:\n",
        "        tf[t] += 1\n",
        "    for t, c in tf.items():\n",
        "        df[t] += 1\n",
        "        postings[t].append((d[\"_id\"], c))\n",
        "\n",
        "index_obj = {\n",
        "    \"N\": N,\n",
        "    \"df\": dict(df),\n",
        "    \"doc_len\": {int(k): int(v) for k, v in doc_len.items()},\n",
        "    \"postings\": {t: [(int(d), int(tf)) for d, tf in lst] for t, lst in postings.items()},\n",
        "}\n",
        "\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "with open(DOCS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    for d in docs:\n",
        "        f.write(json.dumps(d) + \"\\n\")\n",
        "with open(INDEX_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(index_obj, f)\n",
        "\n",
        "print(\"Index written →\", INDEX_JSON)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6e26c6",
      "metadata": {
        "id": "db6e26c6"
      },
      "source": [
        "## 6) Retrieval helper (for your app)\n",
        "\n",
        "BM25-ish search now; swap internals for FAISS/ColBERT and keep the same function signature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "57c6a349",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57c6a349",
        "outputId": "2849bff6-a502-42ee-ca31-aa4804200953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'doc_id': 1, 'score': 0.9996428232642339, 'manual_ref': '2-14', 'title': 'TPMS', 'snippet': 'TPMS monitors tire pressure and warns if it is low.', 'year': None, 'model': None}, {'doc_id': 4, 'score': 0.9996428232642339, 'manual_ref': '2-14', 'title': 'TPMS', 'snippet': 'TPMS monitors tire pressure and warns if it is low.', 'year': 2023, 'model': 'cx9'}, {'doc_id': 7, 'score': 0.9996428232642339, 'manual_ref': '2-14', 'title': 'TPMS', 'snippet': 'TPMS monitors tire pressure and warns if it is low.', 'year': 2025, 'model': '3'}, {'doc_id': 10, 'score': 0.9996428232642339, 'manual_ref': '2-14', 'title': 'TPMS', 'snippet': 'TPMS monitors tire pressure and warns if it is low.', 'year': 2025, 'model': 'cx70'}, {'doc_id': 13, 'score': 0.9996428232642339, 'manual_ref': '2-14', 'title': 'TPMS', 'snippet': 'TPMS monitors tire pressure and warns if it is low.', 'year': 2025, 'model': 'cx70phev'}]\n"
          ]
        }
      ],
      "source": [
        "def load_index(path=INDEX_JSON):\n",
        "    import json\n",
        "    from pathlib import Path\n",
        "    return json.loads(Path(path).read_text())\n",
        "\n",
        "def bm25_search(query, k=5, k1=1.5, b=0.75):\n",
        "    import math, re, json\n",
        "    idx = load_index()\n",
        "    N = idx[\"N\"]\n",
        "    scores = __import__(\"collections\").defaultdict(float)\n",
        "    tokens = re.findall(r\"[a-z0-9_]+\", query.lower())\n",
        "    avgdl = sum(idx[\"doc_len\"].values())/max(1, N)\n",
        "\n",
        "    # Load docs into a dictionary keyed by their _id\n",
        "    doc_map = {}\n",
        "    with open(DOCS_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            doc = json.loads(line)\n",
        "            doc_map[doc[\"_id\"]] = doc\n",
        "\n",
        "    for t in tokens:\n",
        "        df = idx[\"df\"].get(t, 0)\n",
        "        if df == 0:\n",
        "            continue\n",
        "        idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
        "        for doc_id, tf in idx[\"postings\"].get(t, []):\n",
        "            # Retrieve doc length from the doc_map using the correct doc_id\n",
        "            dl = len(re.findall(r\"[a-z0-9_]+\", doc_map[doc_id][\"text_norm\"].lower()))\n",
        "            denom = tf + k1*(1 - b + b*dl/avgdl)\n",
        "            scores[doc_id] += idf * (tf*(k1+1))/denom\n",
        "    top = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
        "    # map to docs\n",
        "    results = []\n",
        "    for doc_id, score in top:\n",
        "        d = doc_map[doc_id]\n",
        "        results.append({\n",
        "            \"doc_id\": doc_id, \"score\": score,\n",
        "            \"manual_ref\": d[\"manual_ref\"], \"title\": d[\"title\"], \"snippet\": d[\"text_norm\"][:240],\n",
        "            \"year\": d.get(\"metadata\", {}).get(\"year\"), # Add year from metadata\n",
        "            \"model\": d.get(\"metadata\", {}).get(\"model\") # Add model from metadata\n",
        "        })\n",
        "    return results\n",
        "\n",
        "print(bm25_search(\"tpms warning light\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d768cab",
      "metadata": {
        "id": "3d768cab"
      },
      "source": [
        "## 7) One-click: Copy artifacts into `/app/` layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "faaf031b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faaf031b",
        "outputId": "969f0d00-828c-46b9-d8e8-3a51d4c39251"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Export complete → /content/drive/MyDrive/AugWorlds/Mazda_PDFs/app\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def export_to_app():\n",
        "    targets = [\n",
        "        (PARSED_DIR, APP_DIR / \"data\" / \"parsed\"),\n",
        "        (ENRICHED_DIR, APP_DIR / \"data\" / \"enriched\"),\n",
        "        (INDEX_DIR, APP_DIR / \"index\"),\n",
        "    ]\n",
        "    for src, dst in targets:\n",
        "        dst.mkdir(parents=True, exist_ok=True)\n",
        "        for p in src.glob(\"**/*\"):\n",
        "            if p.is_file():\n",
        "                rel = p.relative_to(src)\n",
        "                (dst / rel).parent.mkdir(parents=True, exist_ok=True)\n",
        "                shutil.copy2(p, dst / rel)\n",
        "    print(\"Export complete →\", APP_DIR.resolve())\n",
        "\n",
        "export_to_app()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}