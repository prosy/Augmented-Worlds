{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPD9k0rel3PFjRw3HmmkS9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prosy/Augmented-Worlds/blob/main/Mazda_CX_9_Auto_Parse_with_Index_ChatGPT_Claude_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7h3Jj4rWv4t"
      },
      "outputs": [],
      "source": [
        "# Automobile Manual PDF Parser - Complete Extraction for Google Colab\n",
        "# This notebook extracts structured content from Mazda owner's manuals, including TOC, Index, Images, Tables, and Warnings\n",
        "\n",
        "# Step 1: Install Required Libraries\n",
        "!pip install PyMuPDF pdfplumber scikit-image\n",
        "\n",
        "# Step 2: Import Libraries and Mount Google Drive\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import datetime\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files, drive\n",
        "import fitz  # PyMuPDF\n",
        "import pdfplumber\n",
        "from PIL import Image\n",
        "import io\n",
        "import numpy as np\n",
        "import requests\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 3: Define the Parser Class\n",
        "class AutoManualParser:\n",
        "    def __init__(self, pdf_path):\n",
        "        self.pdf_path = pdf_path\n",
        "        self.metadata = self._extract_metadata_from_filename()\n",
        "        self.pdf_doc = fitz.open(pdf_path)\n",
        "        self.extracted_data = {\n",
        "            \"metadata\": self.metadata,\n",
        "            \"toc\": [],\n",
        "            \"sections\": [],\n",
        "            \"images\": [],\n",
        "            \"tables\": [],\n",
        "            \"warnings\": [],\n",
        "            \"index\": []\n",
        "        }\n",
        "\n",
        "    def _extract_metadata_from_filename(self):\n",
        "        filename = os.path.basename(self.pdf_path)\n",
        "        pattern = r'(\\d{4})-([a-z0-9]+)-owners-manual'\n",
        "        match = re.search(pattern, filename)\n",
        "        if match:\n",
        "            year, model = match.groups()\n",
        "            return {\"filename\": filename, \"year\": int(year), \"model\": model, \"page_count\": None}\n",
        "        else:\n",
        "            return {\"filename\": filename, \"year\": None, \"model\": None, \"page_count\": None}\n",
        "\n",
        "    def analyze_document_structure(self):\n",
        "        self.metadata[\"page_count\"] = len(self.pdf_doc)\n",
        "        self.extracted_data[\"metadata\"] = self.metadata\n",
        "        self._extract_toc()\n",
        "        return self.extracted_data\n",
        "\n",
        "    def _extract_toc(self):\n",
        "        toc = self.pdf_doc.get_toc()\n",
        "        if toc:\n",
        "            self.extracted_data[\"toc\"] = [{\"level\": level, \"title\": title, \"page\": page} for level, title, page in toc]\n",
        "\n",
        "    def extract_images(self):\n",
        "        images = []\n",
        "        seen_hashes = set()\n",
        "\n",
        "        for page_num in range(len(self.pdf_doc)):\n",
        "            page = self.pdf_doc[page_num]\n",
        "            image_list = page.get_images(full=True)\n",
        "\n",
        "            for img in image_list:\n",
        "                xref = img[0]\n",
        "                base_image = self.pdf_doc.extract_image(xref)\n",
        "                image_bytes = base_image[\"image\"]\n",
        "                image_hash = hash(image_bytes)\n",
        "\n",
        "                if image_hash in seen_hashes:\n",
        "                    continue\n",
        "                seen_hashes.add(image_hash)\n",
        "\n",
        "                images.append({\n",
        "                    \"page\": page_num + 1,\n",
        "                    \"width\": base_image[\"width\"],\n",
        "                    \"height\": base_image[\"height\"],\n",
        "                    \"image_type\": base_image[\"ext\"]\n",
        "                })\n",
        "\n",
        "        self.extracted_data[\"images\"] = images\n",
        "        return images\n",
        "\n",
        "    def extract_tables(self):\n",
        "        tables = []\n",
        "        with pdfplumber.open(self.pdf_path) as pdf:\n",
        "            for page_num in range(len(pdf.pages)):\n",
        "                page = pdf.pages[page_num]\n",
        "                page_tables = page.extract_tables()\n",
        "\n",
        "                for table_idx, table_data in enumerate(page_tables):\n",
        "                    if table_data:\n",
        "                        clean_data = [['' if cell is None else str(cell).strip() for cell in row] for row in table_data]\n",
        "                        df = pd.DataFrame(clean_data)\n",
        "                        tables.append({\n",
        "                            \"page\": page_num + 1,\n",
        "                            \"table_index\": table_idx,\n",
        "                            \"data\": df.to_dict(orient=\"records\")\n",
        "                        })\n",
        "\n",
        "        self.extracted_data[\"tables\"] = tables\n",
        "        return tables\n",
        "\n",
        "    def extract_index(self):\n",
        "        page_count = len(self.pdf_doc)\n",
        "        start_page = None\n",
        "        index_data = {}\n",
        "\n",
        "        for page_num in range(page_count - 15, page_count):\n",
        "            text = self.pdf_doc[page_num].get_text(\"text\").strip()\n",
        "            if text.lower() == \"index\":\n",
        "                start_page = page_num + 1\n",
        "                break\n",
        "\n",
        "        if start_page is None:\n",
        "            print(\"Index section not found.\")\n",
        "            return\n",
        "\n",
        "        for page_num in range(start_page, page_count):\n",
        "            text = self.pdf_doc[page_num].get_text(\"dict\")\n",
        "            if not text[\"blocks\"]:\n",
        "                break\n",
        "\n",
        "            for block in text[\"blocks\"]:\n",
        "                if \"lines\" in block:\n",
        "                    for line in block[\"lines\"]:\n",
        "                        term, pages = None, []\n",
        "                        for span in line[\"spans\"]:\n",
        "                            if span[\"text\"].strip().isalpha():\n",
        "                                term = span[\"text\"].strip()\n",
        "                            elif span[\"text\"].strip().isdigit():\n",
        "                                pages.append(int(span[\"text\"].strip()))\n",
        "                        if term and pages:\n",
        "                            index_data[term] = pages\n",
        "\n",
        "        self.extracted_data[\"index\"] = index_data\n",
        "        return index_data\n",
        "\n",
        "    def save_results(self):\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        drive_path = f\"/content/drive/MyDrive/parsed_mazda_manual_{timestamp}.json\"\n",
        "        local_path = f\"/content/parsed_mazda_manual_{timestamp}.json\"\n",
        "\n",
        "        with open(drive_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.extracted_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        with open(local_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.extracted_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"Results saved to: {drive_path}\")\n",
        "        print(\"Downloading file...\")\n",
        "        files.download(local_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Locate Your Mazda Manuals in Google Drive and Process All Files\n",
        "mazda_directory = \"/content/drive/MyDrive/Mazda_PDFs/\"\n",
        "processed_files_results = {}\n",
        "\n",
        "if os.path.exists(mazda_directory) and os.path.isdir(mazda_directory):\n",
        "    print(f\"Found your Mazda manual directory: {mazda_directory}\")\n",
        "    pdf_files = [os.path.join(mazda_directory, f) for f in os.listdir(mazda_directory) if f.endswith(\".pdf\")]\n",
        "\n",
        "    if not pdf_files:\n",
        "        print(f\"No PDF files found in {mazda_directory}. Please upload files to this directory.\")\n",
        "    else:\n",
        "        print(f\"Found {len(pdf_files)} PDF files to process.\")\n",
        "        for pdf_filename in pdf_files:\n",
        "            print(f\"Processing file: {pdf_filename}\")\n",
        "            try:\n",
        "                # Step 5: Process the PDF\n",
        "                parser = AutoManualParser(pdf_filename)\n",
        "                print(\"Analyzing document structure...\")\n",
        "                parser.analyze_document_structure()\n",
        "                print(\"Extracting Index...\")\n",
        "                parser.extract_index()\n",
        "                print(\"Extracting Images...\")\n",
        "                parser.extract_images()\n",
        "                print(\"Extracting Tables...\")\n",
        "                parser.extract_tables()\n",
        "                print(\"Saving results...\")\n",
        "                parser.save_results()\n",
        "                processed_files_results[pdf_filename] = \"Success\"\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdf_filename}: {e}\")\n",
        "                processed_files_results[pdf_filename] = f\"Error: {e}\"\n",
        "\n",
        "else:\n",
        "    print(f\"Could not find directory {mazda_directory}. Please ensure the directory exists in your Google Drive.\")\n",
        "    # You might want to add code here to handle the case where the directory doesn't exist,\n",
        "    # perhaps by asking the user to create it or upload files directly.\n",
        "\n",
        "print(\"\\n--- Processing Summary ---\")\n",
        "for filename, status in processed_files_results.items():\n",
        "    print(f\"{filename}: {status}\")\n",
        "\n",
        "print(\"\\nExecution completed!\")"
      ],
      "metadata": {
        "id": "YTn7mJgKzCl1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}